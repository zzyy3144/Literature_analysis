import streamlit as st
import paddle
from paddlenlp.transformers import ErnieTokenizer, ErnieForSequenceClassification
import joblib
import numpy as np
import jieba
import re
import time  # å¯¼å…¥timeæ¨¡å—ç”¨äºè®¡æ—¶

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="æ™ºèƒ½ç§‘ç ”æ–‡çŒ®åˆ†æç³»ç»Ÿ", page_icon="ğŸ“š", layout="wide")

# æ•°æ®é¢„å¤„ç†å‡½æ•°
@st.cache_data
def preprocess_text(text):
    try:
        text = re.sub(r'<.*?>', '', text)  # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'[^\w\s]', '', text)  # å»é™¤ç‰¹æ®Šå­—ç¬¦
        words = jieba.cut(text)  # ä¸­æ–‡åˆ†è¯
        return ' '.join(words)
    except Exception as e:
        st.error(f"æ–‡æœ¬é¢„å¤„ç†æ—¶å‡ºé”™: {e}")
        return text

# å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡
@st.cache_data
def convert_to_tensor(texts, tokenizer, max_seq_length=128):
    try:
        inputs = tokenizer(texts, max_seq_len=max_seq_length, pad_to_max_seq_len=True)
        input_ids = paddle.to_tensor([inputs['input_ids']])
        token_type_ids = paddle.to_tensor([inputs['token_type_ids']])
        return input_ids, token_type_ids
    except Exception as e:
        st.error(f"æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡æ—¶å‡ºé”™: {e}")
        return None, None

# åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨
@st.cache_resource
def load_models():
    try:
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        dt_model = joblib.load('dt_model.pkl')
        ernie_model_state = paddle.load('ernie_model.pdparams')
        ernie_sentiment_model_state = paddle.load('ernie_sentiment_model.pdparams')
        label_encoder_category = joblib.load('label_encoder_category.pkl')
        label_encoder_sentiment = joblib.load('label_encoder_sentiment.pkl')
        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
        st.write(f"æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
        return dt_model, ernie_model_state, ernie_sentiment_model_state, label_encoder_category, label_encoder_sentiment
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æˆ–æ ‡ç­¾ç¼–ç å™¨æ—¶å‡ºé”™: {e}")
        st.stop()

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def initialize_models(ernie_model_state, ernie_sentiment_model_state):
    try:
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')
        model = ErnieForSequenceClassification.from_pretrained('ernie-1.0', num_classes=2)
        sentiment_model = ErnieForSequenceClassification.from_pretrained('ernie-1.0', num_classes=3)
        model.set_state_dict(ernie_model_state)
        sentiment_model.set_state_dict(ernie_sentiment_model_state)
        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
        st.write(f"æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
        return tokenizer, model, sentiment_model
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}")
        st.stop()

# ä¸»å‡½æ•°
def main():
    st.title("æ™ºèƒ½ç§‘ç ”æ–‡çŒ®åˆ†æç³»ç»Ÿ")
    st.markdown("æ¬¢è¿ä½¿ç”¨æ™ºèƒ½ç§‘ç ”æ–‡çŒ®åˆ†æç³»ç»Ÿï¼è¯·è¾“å…¥æ–‡çŒ®æ‘˜è¦è¿›è¡Œåˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æã€‚")

    # ç”¨æˆ·è¾“å…¥
    with st.container():
        text = st.text_area("æ–‡çŒ®æ‘˜è¦", "", height=150)

    if st.button("åˆ†æ"):
        if text:
            try:
                with st.spinner('æ­£åœ¨åˆ†æ...'):
                    # åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨
                    dt_model, ernie_model_state, ernie_sentiment_model_state, label_encoder_category, label_encoder_sentiment = load_models()
                    
                    # åˆå§‹åŒ–æ¨¡å‹
                    tokenizer, model, sentiment_model = initialize_models(ernie_model_state, ernie_sentiment_model_state)
                    
                    # æ–‡æœ¬é¢„å¤„ç†
                    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
                    processed_text = preprocess_text(text)
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    st.write(f"æ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
                    input_ids, token_type_ids = convert_to_tensor([processed_text], tokenizer)
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    st.write(f"å¼ é‡è½¬æ¢å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
                    
                    # æ–‡æœ¬åˆ†ç±»
                    model.eval()
                    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
                    category_logits = model(input_ids, token_type_ids=token_type_ids)
                    category_pred = paddle.argmax(category_logits, axis=1).numpy()[0]
                    category_label = label_encoder_category.inverse_transform([category_pred])[0]
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    st.write(f"æ–‡æœ¬åˆ†ç±»å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
                    
                    # æƒ…æ„Ÿåˆ†æ
                    sentiment_model.eval()
                    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
                    sentiment_logits = sentiment_model(input_ids, token_type_ids=token_type_ids)
                    sentiment_pred = paddle.argmax(sentiment_logits, axis=1).numpy()[0]
                    sentiment_label = label_encoder_sentiment.inverse_transform([sentiment_pred])[0]
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    st.write(f"æƒ…æ„Ÿåˆ†æå®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
                    
                    # æ˜¾ç¤ºç»“æœ
                    with st.container():
                        st.write(f"**åˆ†ç±»ç»“æœ:** {category_label}")
                        st.write(f"**æƒ…æ„Ÿåˆ†æç»“æœ:** {sentiment_label}")
            except Exception as e:
                st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        else:
            st.warning("è¯·è¾“å…¥æ–‡çŒ®æ‘˜è¦ï¼")

if __name__ == "__main__":
    main()